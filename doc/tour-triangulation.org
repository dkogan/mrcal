#+title: A tour of mrcal: triangulation
#+OPTIONS: toc:t

#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{argmin}
#+LATEX_HEADER: \DeclareMathOperator*{\Var}{Var}

#+BEGIN_HTML
\(
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\Var}{Var}
\)
#+END_HTML

This is an overview of a more detailed discussion about [[file:triangulation.org][triangulation methods
and uncertainty]].

* Previous
We just looked at [[file:tour-stereo.org][dense stereo processing]]

* Overview
We just looked at [[file:tour-stereo.org][dense stereo processing]], where we generated a range map: an
image where each pixel encodes a range along each corresponding observation
vector. This is computed relatively efficiently, but computing a range value for
/every/ pixel in the rectified image is still slow, and for many applications it
isn't necessary. The mrcal triangulation routines take the opposite approach:
given a discrete set of observed features, compute the position of just those
features. In addition to being far faster, the triangulation routines propagate
uncertainties and provide lots and lots of diagnostics to help debug
incorrectly-reported ranges.

mrcal's sparse triangulation capabilities are provided by the
[[file:mrcal-triangulate.html][=mrcal-triangulate=]] tool and the [[file:mrcal-python-api-reference.html#-triangulate][=mrcal.triangulate()=]] Python routine. Each of
these ingests

- Some number of pairs of pixel observations $\left\{ \vec q_{0_i}, \vec q_{1_i} \right\}$
- The corresponding camera models
- The corresponding images

To produce

- The position of the point in space $\vec p_i$ that produced these observations
  for each pair
- A covariance matrix, reporting the uncertainty of each reported point $\Var \vec p_i$ and the covariances $\Var \left( \vec p_i, \vec p_j \right)$

* Triangulation
Let's use our Downtown Los Angeles images. Before we start, one important
caveat: there's only one camera, which was calibrated monocularly. The one
camera was moved to capture the two images used to triangulate. The extrinsics
were computed with a not-yet-in-mrcal tool, and mrcal cannot yet propagate the
calibration noise in this scenario. Thus in this example we only propagate the
observation-time noise.

Image from the left camera:

[[file:external/2022-11-05--dtla-overpass--samyang--alpha7/stereo/0.jpg][file:external/figures/stereo/0.downsampled.jpg]]

Let's compute the range to the top of the [[https://en.wikipedia.org/wiki/City_National_Plaza]["Paul Hastings" tower]], near the center
of the image. I'm looking at the "Paul Hastings" logo, roughly 566m (according
to the map) from the camera. I have a pixel coordinate on the logo: $\vec q =
(2874, 1231)$, the two images and the two models. This is enough information to
triangulate:

#+begin_src sh
mrcal-triangulate                         \
    --range-estimate 873                  \
    --q-observation-stdev             0.3 \
    --q-observation-stdev-correlation 0.1 \
    --stabilize                           \
    --template-size 31 17                 \
    --search-radius 10                    \
    --viz uncertainty                     \
    [01].cameramodel                      \
    [01].jpg                              \
    2735 1467
#+end_src
#+begin_src sh :exports none :eval no-export
Dout=~/projects/mrcal-doc-external/figures/triangulation
mkdir -p $Dout

D=~/projects/mrcal-doc-external/2022-11-05--dtla-overpass--samyang--alpha7/stereo
PYTHONPATH=~/projects/mrcal;
export PYTHONPATH;
$PYTHONPATH/mrcal-triangulate                                         \
    --range-estimate 873                                              \
    --q-observation-stdev             0.3                             \
    --q-observation-stdev-correlation 0.1                             \
    --stabilize                                                       \
    --template-size 31 17                                             \
    --search-radius 10                                                \
    --viz uncertainty                                                 \
    --hardcopy $Dout/wilshire-grand-ellipse.svg                       \
    --terminal 'svg size 800,600 noenhanced solid dynamic font ",14"' \
    $D/[01].cameramodel                                               \
    $D/[01].jpg                                                       \
    2735 1467
#+end_src

Here we used the splined models computed [[file:tour-initial-calibration.org::#splined-model-solving][earlier]]. We gave the tool the true
range (566m) to use as a reference. And we gave it the expected observation
noise level: 0.3 pixels (a loose estimate that should be roughly correct). We
declared the left-camera/right-camera pixel observations to be correlated with a
factor of 0.1 on the stdev, so the relevant cross terms of the covariance matrix
are $(0.3*0.1 \mathrm{pixels})^2$. It's not yet clear how to get the true value
of this correlation, but we can use this tool to gauge its effects.

The [[file:mrcal-triangulate.html][=mrcal-triangulate=]] tool finds the corresponding feature in the second image
using a template-match technique in [[file:mrcal-python-api-reference.html#-match_feature][=mrcal.match_feature()=]]. This operation
could fail, so a diagnostic visualization can be requested by passing =--viz
match=. This pops up an interactive window with the matched template overlaid in
its best-fitting position so that a human can validate the match. The match was
found correctly here. We could also pass =--viz uncertainty=, which shows the
uncertainty ellipse. Unless we're looking very close in, this ellipse is almost
always extremely long and extremely skinny. Here we have:

[[file:external/figures/triangulation/wilshire-grand-ellipse.svg]]

So looking at the ellipse usually isn't very useful, and the value printed in
the statistics presents the same information in a more useful way. The
[[file:mrcal-triangulate.html][=mrcal-triangulate=]] tool produces /lots/ of reported statistics:

#+begin_example
## Feature [2735. 1467.] in the left image corresponds to [2817.236 1482.142] at 873.0m
## Feature match found at [2817.699 1481.859]
## q1 - q1_perfect_at_range = [ 0.463 -0.284]
## Triangulated point at [ -88.593 -119.488  960.826]; direction: [-0.091 -0.123  0.988] [camera coordinates]
## Triangulated point at [-90.68 -84.59 964.26]; direction: [-0.091 -0.087  0.992] [reference coordinates]
## Range: 972.27 m (error: 99.27 m)
## q0 - q0_triangulation = [-0.019  0.143]
## Uncertainty propagation: observation-time noise suggests worst confidence of sigma=104.509m along [ 0.092  0.123 -0.988]
## Observed-pixel range sensitivity: 247.157m/pixel (q1). Worst direction: [0.998 0.07 ]. Linearized correction: -0.402 pixels
## Calibration yaw (rotation in epipolar plane) sensitivity: -9003.84m/deg. Linearized correction: 0.011 degrees of yaw
## Calibration yaw (cam0 y axis)                sensitivity: -8854.63m/deg. Linearized correction: 0.011 degrees of yaw
## Calibration pitch (tilt of epipolar plane) sensitivity: 658.85m/deg.
## Calibration translation sensitivity: 530.67m/m. Worst direction: [0.986 0.    0.165]. Linearized correction: -0.19 meters of translation
## Optimized yaw   (rotation in epipolar plane) correction = 0.013 degrees
## Optimized pitch (tilt of epipolar plane)     correction = 0.008 degrees
## Optimized relative yaw (1 <- 0): 0.444 degrees
#+end_example

We see that

- The range we compute here is 972.27m, not 873m as desired
- There's a difference of [-0.019 0.143] pixels between the triangulated point
  and the observation in the left camera: the epipolar lines are aligned well.
  This should be 0, ideally, but 0.143 pixels is easily explainable by pixel
  noise
- With the given observation noise, the 1-sigma uncertainty in the range is
  104.509m, almost exactly in the observation direction. This is very similar to
  the actual error of 99.27m
- Moving the matched feature coordinate in the right image affects the range at
  worst at a rate of 247.157 m/pixel. Unsurprisingly, the most sensitive
  direction of motion is left/right. At this rate, it would take 0.402 pixels of
  motion to "fix" our range measurement
- Similarly, we compute and report the range sensitivity of extrinsic yaw
  (defined as the rotation in the epipolar plane or around the y axis of the
  left camera). In either case, an extrinsics yaw shift of 0.011 degrees would
  "fix" the range measurement.
- We also compute sensitivities for pitch and translation, but we don't expect
  those to affect the range very much, and we see that
- Finally, we reoptimize the extrinsics, and compute a better yaw correction to
  "fix" the range: 0.013 degrees. This is different from the previous value of
  0.011 degrees because that computation used a /linearized/ yaw-vs-range
  dependence

This is all quite useful, and suggests that a small extrinsics error is likely
the biggest problem.

What about =--q-observation-stdev-correlation=? What would be the effect of more
or less correlation in our pixel observations? Running the same command with

- =--q-observation-stdev-correlation 0= (the left and right pixel observations
  are independent) produces

  #+begin_example
## Uncertainty propagation: observation-time noise suggests worst confidence of sigma=105.034m along [ 0.092  0.123 -0.988]
  #+end_example

- =--q-observation-stdev-correlation 1= (the left and right pixel observations
  are perfectly coupled) produces

  #+begin_example
## Uncertainty propagation: observation-time noise suggests worst confidence of sigma=5.757m along [-0.089 -0.147  0.985]
  #+end_example

I.e. correlations in the pixel measurements decrease our range uncertainty. To
the point where perfectly-correlated observations produce almost perfect
ranging. We'll still have range errors, but they would come from other sources
than slightly mismatched feature observations.

A future update to mrcal will include a method to propagate uncertainty through
to re-solved extrinsics and /then/ to triangulation. That will fill-in the
biggest missing piece in the error modeling here.
